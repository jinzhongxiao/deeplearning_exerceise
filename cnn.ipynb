{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0, loss =0.269525940243\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.80380606  0.93660952  0.79029445  0.90889228]]\n",
      "iter = 1000, loss =0.0223421469579\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.06629745  0.21128364  0.20459654  0.70361967]]\n",
      "iter = 2000, loss =0.00690946210576\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.03179877  0.1210863   0.118635    0.84022397]]\n",
      "iter = 3000, loss =0.00339689604832\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.0218827   0.08561778  0.08418313  0.88918884]]\n",
      "iter = 4000, loss =0.00212307956075\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.01736496  0.06791147  0.06694525  0.91288249]]\n",
      "iter = 5000, loss =0.00150529011182\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.01474725  0.05728456  0.05656752  0.92690121]]\n",
      "iter = 6000, loss =0.00115057276106\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.01301421  0.05013786  0.04957145  0.93624988]]\n",
      "iter = 7000, loss =0.000923820946912\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.01176777  0.04496088  0.04449415  0.94298354]]\n",
      "iter = 8000, loss =0.00076779820309\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.01081967  0.04101177  0.04061544  0.94809874]]\n",
      "iter = 9000, loss =0.000654557463302\n",
      "=== Label vs Prediction ===\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.01006894  0.03788318  0.03753902  0.95213808]]\n",
      "=== Final ===\n",
      "X=[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "t=[[0 0 0 1]]\n",
      "y=[[ 0.0094563   0.03533222  0.03502818  0.95542307]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "trace = False\n",
    "trace_forward = False\n",
    "class FC:\n",
    "    def __init__(self, in_num, out_num, lr = 0.001):\n",
    "        self._in_num = in_num\n",
    "        self._out_num = out_num\n",
    "        self.w = np.random.randn(out_num, in_num)\n",
    "        self.b = np.zeros((out_num,1))\n",
    "        self.lr = lr\n",
    "    \n",
    "    def _sigmoid(self, in_data):\n",
    "        return 1/(1 + np.exp(-in_data))\n",
    "    def forward(self, in_data):\n",
    "        # Z = W * X + b    X = sigmoid(Z) \n",
    "        self.topVal = self._sigmoid(np.dot(self.w, in_data) + self.b)\n",
    "        self.bottomVal = in_data\n",
    "        return self.topVal\n",
    "        \n",
    "    def backward(self, loss):\n",
    "        residual_z = loss * self.topVal * (1 - self.topVal)\n",
    "        grad_w = np.dot(residual_z, self.bottomVal.T)\n",
    "        # sum and mean are all right , but sum needs less iteration times\n",
    "        grad_b = np.sum(residual_z)\n",
    "        self.w -= self.lr * grad_w\n",
    "        self.b -= self.lr * grad_b\n",
    "        residual_x = np.dot(self.w.T, residual_z)\n",
    "        return residual_x\n",
    "\n",
    "class SquareLoss:\n",
    "    '''\n",
    "    Same as above, not thread safe\n",
    "    '''\n",
    "    def forward(self, y, t):\n",
    "        self.loss = y - t\n",
    "        if trace:\n",
    "            print '=== Loss ==='.format(self.loss.shape)\n",
    "            print self.loss\n",
    "        return np.sum(self.loss * self.loss) /  self.loss.shape[1] / 2\n",
    "    def backward(self):\n",
    "        if trace:\n",
    "            print '=== loss {0} ==='.format(self.loss.shape)\n",
    "            print self.loss\n",
    "        return self.loss\n",
    "class Net:\n",
    "    def __init__(self, input_num=2, hidden_num=4, out_num = 1, lr = 0.1):\n",
    "        self.fc1 = FC(input_num, hidden_num, lr)\n",
    "        self.fc2 = FC(hidden_num, out_num, lr)\n",
    "        self.loss = SquareLoss()\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        for i in range(10000):\n",
    "            layer1out = self.fc1.forward(X)\n",
    "            layer2out = self.fc2.forward(layer1out)\n",
    "            loss = self.loss.forward(layer2out, y)\n",
    "            if i % 1000 == 0:\n",
    "                print 'iter = {0}, loss ={1}'.format(i, loss)\n",
    "                print '=== Label vs Prediction ==='\n",
    "                print 't={0}'.format(y)\n",
    "                print 'y={0}'.format(layer2out)\n",
    "            layer2loss = self.loss.backward()\n",
    "            layer1loss = self.fc2.backward(layer2loss)\n",
    "            saliency = self.fc1.backward(layer1loss)\n",
    "        layer1out = self.fc1.forward(X)\n",
    "        layer2out = self.fc2.forward(layer1out)\n",
    "        print '=== Final ==='\n",
    "        print 'X={0}'.format(X)\n",
    "        print 't={0}'.format(y)\n",
    "        print 'y={0}'.format(layer2out)\n",
    "        \n",
    "\n",
    "def conv2(X, k):\n",
    "    x_row,x_col = X.shape\n",
    "    k_row,k_col = k.shape\n",
    "    ret_row, ret_col = x_row - k_row + 1, x_col - k_col + 1\n",
    "    ret = np.empty((ret_row,ret_col))\n",
    "    for y in range(ret_row):\n",
    "        for x in range(ret_col):\n",
    "            sub = X[y : y + k_row, x:x+k_col]\n",
    "            ret[y,x] = np.sum(sub * k)\n",
    "    return ret\n",
    "\n",
    "class ConvLayer:\n",
    "    def __init__(self, in_channel, out_channel, kernel_size):\n",
    "        self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)\n",
    "        self.b = np.zeros((out_channel))\n",
    "        \n",
    "    def _relu(self):\n",
    "        x[x<0] = 0\n",
    "        return x\n",
    "    def forward(self, in_data):\n",
    "        in_channel, in_row, in_col = in_data.shape()\n",
    "        out_channel, kernel_row, kernel_col = self.w.shape[1:]\n",
    "        self.top_val = np.zeros((out_channel, in_row - kernel_row + 1, in_col - kernel_col + 1))\n",
    "        for j in range(out_channel):\n",
    "            for i in range(in_channel):\n",
    "                self.top_val[j] += conv2(in_data[i], self.w[i,j])\n",
    "            self.top_val[j] += self.b[j]\n",
    "            self.top_val[j] = self.relu(self.top_val[j])\n",
    "        return self.top_val\n",
    "    \n",
    "    \n",
    "# example from https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "# and operation\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "y = np.array([[0],[0],[0],[1]]).T\n",
    "\n",
    "net = Net(2,4,1,0.1)\n",
    "net.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
